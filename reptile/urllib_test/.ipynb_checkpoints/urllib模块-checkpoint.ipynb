{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### urllib\n",
    "    - python中自带的一个基于爬虫的模块\n",
    "    - 作用：可以使用代码模拟浏览器发起请求 request parse\n",
    "    - 使用流程：\n",
    "       - 指定url\n",
    "       - 发起请求\n",
    "       - 获取页面数据\n",
    "       - 持久化存储"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "写入数据成功\n"
     ]
    }
   ],
   "source": [
    "# 需求：爬取搜狗首页的页面数据\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "#1.指定url\n",
    "url = 'https://www.sogou.com'\n",
    "\n",
    "#2.发起请求：urlopen可以根据指定的url发起请求，且返回一个相应对象\n",
    "response = urllib.request.urlopen(url = url)\n",
    "\n",
    "#3.获取页面数据：read函数返回的就是相应对象中存储的页面数据（byte）\n",
    "page_text = response.read()\n",
    "\n",
    "#4.持久化存储\n",
    "with open(\"./sougou.html\",'wb') as fp:\n",
    "    fp.write(page_text)\n",
    "    print(\"写入数据成功\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据写入成功\n"
     ]
    }
   ],
   "source": [
    "# 需求：爬取指定词条所对应的数据\n",
    "\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "\n",
    "#指定url\n",
    "url = 'https://www.sogou.com/web?query='\n",
    "#url特性：url不可以存在非ASCII编码的字符数据。quoto可将非ASCII编码的字符转换为ASCII码\n",
    "word = urllib.parse.quote(\"人民币\")\n",
    "url += word\n",
    "\n",
    "#发起请求\n",
    "response = urllib.request.urlopen(url)\n",
    "\n",
    "#获取页面数据\n",
    "page_text = response.read()\n",
    "\n",
    "#持久化存储\n",
    "with open ('./renmingbi.html','wb') as fb:\n",
    "    fb.write(page_text)\n",
    "    print(\"数据写入成功\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 反爬机制：网站检查请求的UA，如果发现UA是爬虫程序，则拒绝提供网站数据\n",
    "- User-Agent(UA):请求载体的身份标识\n",
    "- 反反爬机制：伪装爬虫程序请求的UA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "写入数据成功\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "url = 'http://www.baidu.com'\n",
    "\n",
    "#UA伪装\n",
    "#1.自指定一个请求对象\n",
    "headers = {\n",
    "    #存储任意的请求头信息\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36'\n",
    "}\n",
    "#该请求对象的UA进行了成功的伪装\n",
    "request  = urllib.request.Request(url = url,headers = headers)\n",
    "\n",
    "#2.针对自定制的请求对象发起请求\n",
    "response = urllib.request.urlopen(request)\n",
    "\n",
    "#3.获取页面数据\n",
    "page_text = response.read()\n",
    "\n",
    "#4.持久化存储\n",
    "with open('./baidu.html','wb') as fb:\n",
    "    fb.write(page_text)\n",
    "    print(\"写入数据成功\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- urllib模块发起的post请求\n",
    "   - 需求：爬取百度翻译的翻译结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据写入成功\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import urllib.parse\n",
    "\n",
    "#1.指定url\n",
    "url = 'https://fanyi.baidu.com/sug'\n",
    "\n",
    "#post请求携带的参数处理流程：\n",
    "#1）将post请求参数封装到字典\n",
    "data = {\n",
    "    'kw':'西瓜'\n",
    "}\n",
    "#2）使用parse模块中的urlencode（返回值类型为str）进行编码处理\n",
    "data = urllib.parse.urlencode(data)\n",
    "\n",
    "#3)将步骤2的编码结果转换成byte类型\n",
    "data = data.encode()\n",
    "\n",
    "\n",
    "#2.针对url发起请求:发起post请求:urlopen函数的data参数表示的就是经过处理之后的post请求参数\n",
    "response = urllib.request.urlopen(url = url , data = data)\n",
    "\n",
    "#3.获取页面数据\n",
    "page_text = response.read()\n",
    "#4.持久化存储\n",
    "with open('./baidutranslation.json','wb') as fb:\n",
    "    fb.write(page_text)\n",
    "    print(\"数据写入成功\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
